{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StarMindz/Pinterest-Image-Scrapper/blob/main/Image_Scrapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pinterest Image Scrapper"
      ],
      "metadata": {
        "id": "YTBifAz2W55J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script is designed for scraping both high-quality and low-quality images from the internet using a set of search terms. It includes scripts for scraping images from Pinterest. You can specify the number of images you wish to scrape and the directory on your computer where you'd like to store them. For each search term in your list, a folder will be automatically created with the same name as the search term, and the specified number of images will be downloaded automatically.\n",
        "\n",
        "Here, a list of popular Nigerian foods is used, making this script useful for gathering image datasets for machine learning training and general AI model training."
      ],
      "metadata": {
        "id": "9__y8aXgXJ-N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qw6bIVSEdpW6",
        "outputId": "2c1e964b-a1ed-4a2b-bd61-dee5f7193978"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.22.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.25.1-py3-none-any.whl (467 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.7/467.7 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.6.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.7)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.22.0 trio-0.25.1 trio-websocket-0.11.1 wsproto-1.2.0\n",
            "Collecting selenium_stealth\n",
            "  Downloading selenium_stealth-1.0.6-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (from selenium_stealth) (4.22.0)\n",
            "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium->selenium_stealth) (2.0.7)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium->selenium_stealth) (0.25.1)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium->selenium_stealth) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium->selenium_stealth) (2024.6.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from selenium->selenium_stealth) (4.12.2)\n",
            "Requirement already satisfied: websocket-client>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from selenium->selenium_stealth) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->selenium_stealth) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->selenium_stealth) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->selenium_stealth) (3.7)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->selenium_stealth) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->selenium_stealth) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->selenium_stealth) (1.2.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium->selenium_stealth) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium->selenium_stealth) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium->selenium_stealth) (0.14.0)\n",
            "Installing collected packages: selenium_stealth\n",
            "Successfully installed selenium_stealth-1.0.6\n"
          ]
        }
      ],
      "source": [
        "!pip install selenium\n",
        "!pip install selenium_stealth\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!import pillow"
      ],
      "metadata": {
        "id": "msX5U1gjlV5N",
        "outputId": "f5b67069-3bd2-4664-b02f-4638fc8f5e56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: import: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYQMEE3zazrL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from PIL import Image\n",
        "import io\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJcjd0_Hetvb"
      },
      "outputs": [],
      "source": [
        "nigerian_dishes = [\n",
        "    \"Jollof Rice\", \"Pounded Yam\", \"Egusi Soup\", \"Meat\", \"Akara\", \"Efo Riro\", \"Banga Soup\", \"Ofada Rice and Ofada Stew\",\n",
        "    \"Edikang Ikong Soup\", \"Amala\", \"Ogbono Soup\", \"Gbegiri and Ewedu Soup\", \"Nkwobi\", \"Afang Soup\", \"Tuwon Shinkafa\", \"Fried Plantain\", \"Miyan Taushe\",\n",
        "    \"Oha Soup\", \"Masa Waina\", \"Beans and Plantain\", \"Bitterleaf Soup\", \"Ofe Nsala\",\n",
        "   \"Yam Porridge\", \"Vegetable soup\", \"Ogbono Soup\", \"Okra soup\",\"Pepper Soup\"\n",
        "    \"Puff-Puff\",\"Chin Chin\", \"Plantain Chips\", \"Meat pie\", \"Fish roll\", \"Egg Roll\", \"Spring Roll\", \"Samosa\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmPJahLqlu1z"
      },
      "outputs": [],
      "source": [
        "pinterest_base_url = \"https://www.pinterest.com/search/pins/?q=\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bGz-9EtmaH1"
      },
      "outputs": [],
      "source": [
        "# Prepare the browser\n",
        "def prepare_browser():\n",
        "    chrome_options = webdriver.ChromeOptions()\n",
        "    chrome_options.add_argument(\"start-maximized\")\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "    return driver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCbUruZiedDR"
      },
      "outputs": [],
      "source": [
        "# Download images\n",
        "def download_images(images, search_term, save_dir, max_images=1000):\n",
        "    count = len(os.listdir(save_dir))  # Start counting from the current number of files in the directory\n",
        "    for img in images:\n",
        "        if count >= max_images:\n",
        "            break\n",
        "        src = img.get_attribute('src')\n",
        "        if src and 'https' in src:  # Ensure the src attribute is valid\n",
        "            try:\n",
        "                img_data = requests.get(src).content\n",
        "                timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S%f\")\n",
        "                img_file = os.path.join(save_dir, f\"{search_term}_{timestamp}.jpg\")\n",
        "                with open(img_file, 'wb') as handler:\n",
        "                    handler.write(img_data)\n",
        "                count += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Could not download {src}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_single_image(image_url, search_term, save_dir):\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    if image_url and 'https' in image_url:\n",
        "        try:\n",
        "            img_data = requests.get(image_url).content\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S%f\")\n",
        "            img_file = os.path.join(save_dir, f\"{search_term}_{timestamp}.jpg\")\n",
        "            with open(img_file, 'wb') as handler:\n",
        "                handler.write(img_data)\n",
        "            print(f\"Image downloaded: {img_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not download {image_url}: {e}\")"
      ],
      "metadata": {
        "id": "noKHioMqY3Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wait_and_click(driver, by, value):\n",
        "    try:\n",
        "        # Wait for the element to be clickable\n",
        "        element = WebDriverWait(driver, 10).until(\n",
        "            EC.element_to_be_clickable((by, value))\n",
        "        )\n",
        "        # Scroll into view and click using JavaScript\n",
        "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", element)\n",
        "        driver.execute_script(\"arguments[0].click();\", element)\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "id": "TZSqPwU4VKqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOh7USx1eUBx"
      },
      "outputs": [],
      "source": [
        "# Scrape images for a search term\n",
        "def scrape_low_quality_images(driver, search_term, save_dir, max_images=1000):\n",
        "    url = f'https://www.pinterest.com/search/pins/?q={search_term}'\n",
        "    driver.get(url)\n",
        "    print(f\"Searching for {search_term} on Pinterest\")\n",
        "\n",
        "    # Scroll down to load more images\n",
        "    scroll_pause_time = 2\n",
        "    while True:\n",
        "        images = driver.find_elements(By.TAG_NAME, 'img')\n",
        "        print(\"Test\", images)\n",
        "        download_images(images, search_term, save_dir, max_images)\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        time.sleep(scroll_pause_time)\n",
        "        if len(os.listdir(save_dir)) >= max_images:\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_high_quality_images(driver, search_term, save_dir, max_images=1000):\n",
        "    url = f'https://www.pinterest.com/search/pins/?q={search_term}'\n",
        "    driver.get(url)\n",
        "    print(f\"Searching for {search_term} on Pinterest\")\n",
        "    time.sleep(10)\n",
        "\n",
        "    downloaded_count = 0\n",
        "    pins_seen = set()\n",
        "    image_srcs = set()\n",
        "\n",
        "    while downloaded_count < max_images:\n",
        "        # Collect visible thumbnail XPaths\n",
        "        images = [img for img in driver.find_elements(By.XPATH, '//div[@data-test-id=\"pin\"]//img') if img.is_displayed()]\n",
        "\n",
        "        for img in images:\n",
        "            try:\n",
        "                print(\"I got here\")\n",
        "                img_source = img.get_attribute('src')\n",
        "                print(\"Current src\", img_source)\n",
        "                image_srcs.add(img_source)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        print(\"The image srcs \", image_srcs)\n",
        "\n",
        "        # Process each image\n",
        "        for img_src in image_srcs:\n",
        "            if downloaded_count >= max_images:\n",
        "                break\n",
        "\n",
        "            if img_src not in pins_seen:\n",
        "                print(\"Collected Image \", img_src)\n",
        "                # Scroll to the image to make it visible\n",
        "                # scroll_to_element(driver, img)\n",
        "                wait_and_click(driver, By.XPATH, f'//img[@src=\"{img_src}\"]')\n",
        "                time.sleep(3)  # Wait for the pin to open\n",
        "\n",
        "                # Re-locate the full image element\n",
        "                try:\n",
        "                    full_image = WebDriverWait(driver, 10).until(\n",
        "                        EC.presence_of_element_located((By.XPATH, '//img[contains(@class, \"hCL kVc L4E MIw\") and not(@elementtiming=\"closeupImage\")]'))\n",
        "                    )\n",
        "                except:\n",
        "                    pass\n",
        "                try:\n",
        "                    full_image_url = full_image.get_attribute('src')\n",
        "                    print(\"The full image src\", full_image_url)\n",
        "                    download_single_image(full_image_url, search_term, save_dir)\n",
        "                    downloaded_count += 1\n",
        "                    pins_seen.add(img_src)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "                driver.execute_script(\"window.history.go(-1)\")\n",
        "                time.sleep(2)  # Wait for the page to go back\n",
        "\n",
        "        # Clear processed image_srcs and load more content if necessary\n",
        "        image_srcs.clear()\n"
      ],
      "metadata": {
        "id": "Fpb1dMsdaYWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function\n",
        "def main():\n",
        "    driver = prepare_browser()\n",
        "    base_dir = r'C:\\Users\\STARMINDS\\Desktop\\Projects\\Data' # Specify your base directory here\n",
        "    os.makedirs(base_dir, exist_ok=True)\n",
        "    for dish in nigerian_dishes:\n",
        "        save_dir = os.path.join(base_dir, dish)\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        scrape_high_quality_images(driver, dish, save_dir, max_images)\n",
        "    driver.quit()"
      ],
      "metadata": {
        "id": "0vONkgxOo5nB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_images = 4"
      ],
      "metadata": {
        "id": "iduCD3nLo8RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "MeqWe-rXo-k2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5_dZ863eZJJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MBzhebdrV8FB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUYoSolu9ZYbQ8XgjtVqEk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}